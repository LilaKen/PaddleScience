W1030 13:25:43.721320 1222579 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.2, Runtime API Version: 12.0
W1030 13:25:43.721853 1222579 gpu_resources.cc:164] device: 0, cuDNN Version: 9.5.
Dataloading is over.
Namespace(lr=0.001, epochs=500, weight_decay=1e-05, model='Transolver_Structured_Mesh_2D', n_hidden=128, n_layers=8, n_heads=8, batch_size=4, gpu=0, max_grad_norm=0.1, downsample=5, mlp_ratio=1, dropout=0.0, ntrain=1000, unified_pos=1, ref=8, slice_num=64, eval=1, save_name='darcy_UniPDE', data_path='data/fno')
Model(
  (preprocess): MLP(
    (linear_pre): Sequential(
      (0): Linear(in_features=65, out_features=256, dtype=None)
      (1): GELU(approximate=False)
    )
    (linear_post): Linear(in_features=256, out_features=128, dtype=None)
    (linears): LayerList()
  )
  (blocks): LayerList(
    (0): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (1): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (2): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (3): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (4): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (5): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (6): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
    )
    (7): Transolver_block(
      (ln_1): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (Attn): Physics_Attention_Structured_Mesh_2D(
        (softmax): Softmax(axis=-1)
        (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        (in_project_x): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_fx): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (in_project_slice): Linear(in_features=16, out_features=64, dtype=None)
        (to_q): Linear(in_features=16, out_features=16, dtype=None)
        (to_k): Linear(in_features=16, out_features=16, dtype=None)
        (to_v): Linear(in_features=16, out_features=16, dtype=None)
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): Dropout(p=0.0, axis=None, mode=upscale_in_train)
        )
      )
      (ln_2): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp): MLP(
        (linear_pre): Sequential(
          (0): Linear(in_features=128, out_features=128, dtype=None)
          (1): GELU(approximate=False)
        )
        (linear_post): Linear(in_features=128, out_features=128, dtype=None)
        (linears): LayerList()
      )
      (ln_3): LayerNorm(normalized_shape=[128], epsilon=1e-05)
      (mlp2): Linear(in_features=128, out_features=1, dtype=None)
    )
  )
)
Total Trainable Params: 2826945
model evaluation
85 85
W1030 13:26:02.418881 1222579 multiply_fwd_func.cc:64] got different data type, run type protmotion automatically, this may cause data type been changed.
1
2
3
4
5
6
7
8
9
rel_err:0.00567440442168881